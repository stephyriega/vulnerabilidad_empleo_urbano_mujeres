{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e36991f",
   "metadata": {},
   "source": [
    "# Thesis project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfdec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas openpyxl\n",
    "# !pip install graphviz pretty_confusion_matrix pydotplus shap kneed\n",
    "# !pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005a939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.1\n",
    "threshold=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d515c",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007196c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }a\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }a\n",
    "</style>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75445578",
   "metadata": {},
   "outputs": [],
   "source": [
    "1#general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22c646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for Lasso & Data Splitting\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300915ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658f75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages for \n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9906771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import  confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6c94445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53e4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn import pipeline as pl \n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad9bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acd0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a289d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedGroupKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "007c6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from pretty_confusion_matrix import pp_matrix_from_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4f6ef",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313d99c",
   "metadata": {},
   "source": [
    "Here, we import the dataframe that is composed of XX observations and XX variables. It is composed of observation of individual in a year $t$ and $t+1$. In case the person appeared consecutively, \n",
    "\n",
    "i) We include only the first two years they were included. If we did include them all, we would bias the model training them in the observations that repeated the most - that is, the individuals that were surveyed in more than year. In comparison to a normal econometric estimation where representation matter, we don't need or want to ensure that the sample we have allow us to talk in ceratin levels.\n",
    "\n",
    "ii) We included all the first observstions of the appended panels. That is, if the person was followed by 4 years, then we have the person in the 3 initial years of a dual panel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8cdd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import data \n",
    "data_1 = pd.read_excel(\"G:\\\\Mi unidad\\\\PUCP\\\\2021-2\\\\TESIS_1\\\\3_datos\\\\enaho_final_one_python_230823.xlsx\")\n",
    "data=data_1\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Define the number of random observations you want to select\n",
    "# num_observations_to_select = 500  # Change this number as needed\n",
    "\n",
    "# # Use random.sample() to select random observations\n",
    "# data_2 = data_1.sample(n= num_observations_to_select, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5de900",
   "metadata": {},
   "outputs": [],
   "source": [
    " data_1.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ca20c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# col = ['deg_desemp', 'regnat', 'ano_reg', 'num_panel', 'p203a', 'p5561a', 'p5562a', 'p5566a', 'estrato', 'p5571a', 'ano']\n",
    "# data=data_2.loc[:, col]\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d3a3a",
   "metadata": {},
   "source": [
    "### Missings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d0f8d",
   "metadata": {},
   "source": [
    "Drop variables that have a percentage of missings values superior to threshold (0.1 are going to be created after OHE). We do this to keep the variables that are more answered since they are going to be the ones that are going to be more available in the surveys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the missing threshold and output file name\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the variable names\n",
    "vars_delete_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_delete_val = data.columns[data.apply(lambda x: sum(x.isna())/len(x)) > missing_threshold]\n",
    "vars_delete_all += list(vars_delete_val)\n",
    "num_elements = len(vars_delete_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d7d3c",
   "metadata": {},
   "source": [
    "We can see that the mostly the variables that are delimited are categorical/continuos variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accb5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brief analysis of the variables that are dropped in the missing section\n",
    "\n",
    "print('Numeros de variables con mas de 10% de missings: {:.0f}'.format(num_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38bb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean_m=data.drop(vars_delete_all, axis=1)\n",
    "data_clean_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_m.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0992b2",
   "metadata": {},
   "source": [
    "### Dropeamos outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befa59e",
   "metadata": {},
   "source": [
    "In order to keep the values that are more adjusted to medium reality, we choose to drop the outliers of the continuos variables. We set lower and upper thresholds with 1 standar deviation from the mean of the variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that replaces with missing the value of that variable in that observation in case they are below or above \n",
    "# the lower and upper threshold (respectively).\n",
    "\n",
    "def replace_outliers_with_threshold(df, cols):\n",
    "    for col in cols:\n",
    "        # Calculate the mean and standard deviation for the current column\n",
    "        col_mean = df[col].mean()\n",
    "        col_std = df[col].std()\n",
    "\n",
    "        # Calculate the lower and upper thresholds (3 standard deviations away from the mean)\n",
    "        lower_threshold = col_mean - 1 * col_std\n",
    "        upper_threshold = col_mean + 1 * col_std\n",
    "\n",
    "        # Replace outliers with the threshold value\n",
    "        df[col] = np.where((df[col] < lower_threshold) | (df[col] > upper_threshold), col_mean, df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the names of continuous variables\n",
    "# continuous_cols = []\n",
    "\n",
    "# # Set a threshold for the number of unique values to distinguish categorical from continuous variables\n",
    "# unique_value_threshold = 8\n",
    "\n",
    "# # Iterate through the columns and identify continuous variables\n",
    "# for col in data_clean_m.columns:\n",
    "#     # Check if the column is numeric\n",
    "#     if data_clean_m[col].dtype in [int, float]:\n",
    "#         # Check the number of unique values in the column\n",
    "#         num_unique_values = data_clean_m[col].nunique()\n",
    "        \n",
    "#         # Consider it as continuous only if the number of unique values is above the threshold\n",
    "#         if num_unique_values > unique_value_threshold:\n",
    "#             continuous_cols.append(col)\n",
    "\n",
    "# # Now 'continuous_cols' will contain the names of continuous variables in the DataFrame\n",
    "# print(continuous_cols)\n",
    "\n",
    "# continuous_cols = ['y_sec', 'y_mkt','pobre2', 'gpcm', 'ingtrabw', 'ipcr_0', 'ipcr_1', 'ipcr_2', 'ipcr_3', 'ipcr_4', 'ipcr_5', 'ipcr_6', 'ipcr_7', 'ipcr_8', 'ipcr_9', 'ipcr_10', 'ipcr_11', 'ipcr_12',\n",
    "#     'ipcr_13', 'ipcr_14', 'ipcr_15', 'ipcr_16', 'ipcr_17', 'ipcr_18', 'ipcr_19', 'ipcr_20', 'gpgru1', 'gpgru2', 'gpgru3', 'gpgru4', 'gpgru5', 'gpgru6', 'gpgru7', 'gpgru8', 'gpgru9','gpgru10', 'p104a', \n",
    "#     'i1172_01', 'i1172_02', 'i1173_01', 'i1173_02', 'p208a', 'n_edad_prim', 'n_edad_sec', 'n_edad_esc', 'n_matr_prim', 'n_matr_sec', 'n_matr_esc'] #con 0.1\n",
    "\n",
    "# continuous_cols = [ 'y_pri',  'y_sec', 'y_mkt', 'pobre2', 'gpcm', 'ingtrabw', 'ipcr_0', 'ipcr_1', 'ipcr_2', 'ipcr_3', 'ipcr_4',\n",
    "#     'ipcr_5', 'ipcr_6', 'ipcr_7', 'ipcr_8', 'ipcr_9', 'ipcr_10', 'ipcr_11', 'ipcr_12', 'ipcr_13', 'ipcr_14', 'ipcr_15', 'ipcr_16', 'ipcr_17', 'ipcr_18', 'ipcr_19', 'ipcr_20',\n",
    "#     'gpgru1', 'gpgru2', 'gpgru3', 'gpgru4', 'gpgru5', 'gpgru6', 'gpgru7', 'gpgru8', 'gpgru9', 'gpgru10', 'p104a', 'i1172_01', 'i1172_02', 'i1173_01', 'i1173_02', 'p208a', 'p512a', 'p513t',\n",
    "#     'n_edad_prim', 'n_edad_sec', 'n_edad_esc', 'n_matr_prim', 'n_matr_sec', 'n_matr_esc'] #0.5\n",
    "\n",
    "\n",
    " \n",
    "cat_col_total = [\n",
    "    'p300a', 'sector', 'educ', 'dpto', 'regnat', 'tamahno', 'ciiu_1d', 'ciiu_6c',\n",
    "    'p101', 'p102', 'p103', 'p103a', 'p105a', 'p110', 'p111a', 'p203', 'est_civil', 'p302',\n",
    "    'p506r4', 'p510a1', 'p511a', 'p512b', 'p523', 'emplpsec', 'p517','p558c', 'estrato','ano',  'p599'\n",
    "]\n",
    "\n",
    "continous_col_total = [\n",
    "    'y_pri_dep', 'y_pri_indep', 'y_pri', 'y_sec_dep', 'y_sec_ind', 'y_sec', 'y_mkt',\n",
    "    'pobre2', 'gpcm', 'ingtrabw', 'ipcr_0', 'ipcr_1', 'ipcr_2', 'ipcr_3', 'ipcr_4',\n",
    "    'ipcr_5', 'ipcr_6', 'ipcr_7', 'ipcr_8', 'ipcr_9', 'ipcr_10', 'ipcr_11', 'ipcr_12',\n",
    "    'ipcr_13', 'ipcr_14', 'ipcr_15', 'ipcr_16', 'ipcr_17', 'ipcr_18', 'ipcr_19', 'ipcr_20',\n",
    "    'gpgru1', 'gpgru2', 'gpgru3', 'gpgru4', 'gpgru5', 'gpgru6', 'gpgru7', 'gpgru8', 'gpgru9',\n",
    "    'gpgru10', 'p104a', 'i1172_01', 'i1172_02', 'i1173_01', 'i1173_02', 'p208a', 'p512a', 'p513t',\n",
    "    'n_edad_prim', 'n_edad_sec', 'n_edad_esc', 'n_matr_prim', 'n_matr_sec', 'n_matr_esc','p203a'\n",
    "]\n",
    "\n",
    "\n",
    "binary_col_total = [\n",
    "    'p401h1', 'p401h2', 'p401h3', 'p401h4', 'p401h5', 'p401h6', 'p4191', 'p4192', 'p4193', 'p4194', 'p4195', \n",
    "    'p4196', 'p4197', 'p4198', 'p1141', 'p1142', 'p1143', 'p1144', 'p1145', 'ocupinf', 'p501', 'p401c', 'p401f',\n",
    "    'p306', 'p314a', 'p204', 'nbi1_pobre', 'p106a', 'p104b1', 'p104b2', 'p22', 'p_relativa', 'p558e6', \n",
    "    'celular', 'analfa','menor_trabaja'\n",
    "]\n",
    "\n",
    "\n",
    "id_col_total = [\n",
    "     'num_panel','ano_reg', 'deg_desemp'\n",
    "]\n",
    "\n",
    "\n",
    "total = cat_col_total + continous_col_total + binary_col_total + id_col_total\n",
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cont_total = list(set(continous_col_total).intersection(data_clean_m.columns))\n",
    "f_cat_total= list(set(cat_col_total).intersection(data_clean_m.columns))\n",
    "f_bin_total = list(set(binary_col_total).intersection(data_clean_m.columns))\n",
    "#f_id_total = list(set(id_col_total).intersection(data_clean_m.columns)\n",
    "\n",
    "data_clean_m[f_bin_total] =data_clean_m[f_bin_total].replace({2: 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcb89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the functions to the selected variables\n",
    "df_cleaned = replace_outliers_with_threshold(data_clean_m.copy(), f_cont_total)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b511ab",
   "metadata": {},
   "source": [
    "We check that the dataset is different from the previous one, to know we have change it (there are not outliers in the continous variables anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_clean_m.copy().equals(df_cleaned):\n",
    "    print(\"The two DataFrames are identical.\")\n",
    "else:\n",
    "    print(\"The two DataFrames are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb0bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21581c1",
   "metadata": {},
   "source": [
    "### One hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea70a899",
   "metadata": {},
   "source": [
    "Now, we hace to one-hot-encode the categorical variables in order to introduce them to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f776b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the specified columns\n",
    "df_encoded = pd.get_dummies(df_cleaned, columns=f_cat_total )\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68b888",
   "metadata": {},
   "source": [
    "We checked whether a fundamental rule is accomplished: n >> p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_clean_m.shape[0] >= data_clean_m.shape[1]:\n",
    "    print(\"The number of observations is superior than the number of variables.\")\n",
    "else:\n",
    "    print(\"The number of observations is NOT superior than the number of variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc71604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03181596",
   "metadata": {},
   "outputs": [],
   "source": [
    "ano_columns = df_encoded.filter(regex='^ano_reg')\n",
    "print(ano_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db31e62",
   "metadata": {},
   "source": [
    "### Stratifying data for regnatxyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97176997",
   "metadata": {},
   "source": [
    "The variable that we are going to be using for the stratification in the CV stratified is going to be the interaction between the natural region  (3) and the year of the observation (4). We choose to do this because eventhough we cannot not talk about representation, we can at least affirm that the stratas are different from each other (they have, at least, 0.15 of covariance).\n",
    "\n",
    "We did a previous analysis os the interaction between departments (25) and the year (4) of the observation, but we couldn't find the minimun level of covariance as in the previous interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X   = df_encoded.drop(['deg_desemp', 'ano_16', 'ano_17', 'ano_18', 'ano_19', 'regnat_1', 'regnat_2', 'regnat_3','ano_reg'], axis=1)\n",
    "data_Y   = df_encoded.loc[:,['deg_desemp', 'ano_16', 'ano_17', 'ano_18', 'ano_19', 'regnat_1', 'regnat_2', 'regnat_3','ano_reg']]\n",
    "data_X_m = df_encoded.loc[:, ['ano_16', 'ano_17', 'ano_18', 'ano_19', 'regnat_1', 'regnat_2', 'regnat_3', 'num_panel', 'ano_reg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ccae51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ea5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X['p204'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brief analysis of the variables that are dropped in the missing section\n",
    "\n",
    "print('The number of observations is {:.0f}'.format(data_X.shape[0]))\n",
    "print('While the number of variables is {:.0f}'.format(data_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ffef2",
   "metadata": {},
   "source": [
    "## Dropping correlated variables or features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff085a1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_X_corr = data_X.corr()\n",
    "data_X_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named data_X_corr\n",
    "row_values = data_X_corr.filter(regex='^y_')\n",
    "\n",
    "# Print or use the values from the first row\n",
    "print(row_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09bd7c",
   "metadata": {},
   "source": [
    "Here, we select the pair of variables that have a correlation higher than $0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfa453",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = np.full((data_X_corr.shape[0]+1,2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are going to create the pairs of variables from the dataframe which absolute value of correlation is superior to 0.8. \n",
    "pairs = np.empty((0, 2), dtype=int)\n",
    "\n",
    "for i in range(data_X_corr.shape[0]):\n",
    "    for x in range(1, data_X_corr.shape[0]):\n",
    "        for j in range(i + x, data_X_corr.shape[0]):\n",
    "            if (data_X_corr.iloc[i, j] >= threshold) | (data_X_corr.iloc[i, j] <= -threshold):\n",
    "                pairs = np.vstack((pairs, [i, j]))\n",
    "                pairs_unique = np.unique(pairs, axis=0)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900339bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it could happen that one variable is correlated with more than one.  \n",
    "delete_unique = np.unique(np.ravel(pairs_unique))\n",
    "delete_columns = delete_unique.reshape(1, -1)\n",
    "print(delete_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((pairs_unique.shape[0],1), 0, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, we keep the variable that is more related to the outcome variable.\n",
    "for i in range(pairs_unique.shape[0]):\n",
    "    column_names_i = data_X_corr.columns[pairs_unique[i]]\n",
    "    merged_i = pd.concat([data_X[column_names_i], data_Y[\"deg_desemp\"]], axis=1)\n",
    "    data_X_corr_2 = merged_i.corr()    \n",
    "    if (abs(data_X_corr_2.iloc[0,2]) > abs(data_X_corr_2.iloc[1,2])):\n",
    "        #print(f\"i,j is {pairs_unique[i]} because {abs(data_X_corr_2.iloc[0,2])} > {abs(data_X_corr_2.iloc[1,2])}\")\n",
    "        columns[i]= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74964701",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_array = np.column_stack((pairs_unique, columns))\n",
    "\n",
    "win = np.full((merged_array.shape[0],1), 0)\n",
    "lose =  np.full((merged_array.shape[0],1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(merged_array.shape[0]):\n",
    "    if merged_array[i, 2] == 1:\n",
    "            win[i] = merged_array[i, 0]\n",
    "            lose[i] = merged_array[i, 1]\n",
    "    else:\n",
    "            win[i] = merged_array[i, 1]\n",
    "            lose[i] = merged_array[i, 0]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_filter = np.column_stack((merged_array, win, lose))\n",
    "filter = np.full((merged_filter.shape[0],1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79abe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(merged_filter.shape[0]):\n",
    "    for x in range( merged_filter.shape[0]):\n",
    "        if merged_filter[i, 3] == merged_filter[x, 4]:\n",
    "            #print(f\"{merged_filter[i, 3]} is repeated in column {x}\")\n",
    "            filter[i]= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62311ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = np.column_stack((merged_filter,filter))\n",
    "merged_final = merged_final[merged_final[:, -1] == 0]\n",
    "\n",
    "keep_unique = np.unique(np.ravel(merged_final[:,3]))\n",
    "keep_columns = keep_unique.reshape(1, -1)\n",
    "print(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column_names = data_X_corr.columns[np.squeeze(delete_columns)]\n",
    "keep_column_names = data_X_corr.columns[np.squeeze(keep_columns)]\n",
    "keep_column_names_with_index = keep_column_names.tolist() + [\"num_panel\"]\n",
    "data_X_keep = data_X.loc[:, keep_column_names_with_index ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ac475",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.drop(drop_column_names, axis=1, inplace=True)\n",
    "data_X_f = pd.merge(data_X, data_X_keep)\n",
    "data_X_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece9657",
   "metadata": {},
   "source": [
    "#### Quitamos observaciones con missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28509d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_X_f.index.equals(data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c58cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values from data_X_f\n",
    "data_X_f_s = data_X_f.dropna()\n",
    "\n",
    "data_X_f_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of dropped rows\n",
    "dropped_indices = data_X_f.index.difference(data_X_f_s.index)\n",
    "\n",
    "# Drop corresponding rows from data_Y using the dropped indices\n",
    "data_Y_s = data_Y.drop(dropped_indices)\n",
    "data_X_m_s = data_X_m.drop(dropped_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ac2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X_m_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nota: no importa si estan correlacionadas pero en negativo puesto que aportan de manera diferente al Y\n",
    "# | (data_X_corr.iloc[i,j] <= -threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e01654",
   "metadata": {},
   "source": [
    "We stablish a thershold of 0.8. We keep 165 variables.But then, we add the ones corresponding to the year (3) and the ones corresponding to the regions (25)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d368f",
   "metadata": {},
   "source": [
    "The variables we dropped were: . Now, we merge this dataset with the variables we will use for the CV stratified: ano, dep, ano_dep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = pd.merge(data_X_f, data_X_m_s, left_index=True, right_index=True)\n",
    "# Drop 'num_panel_x' and 'ano_reg' columns from data_m\n",
    "data_m.drop(columns=['num_panel_x'], inplace=True)\n",
    "\n",
    "# Rename 'num_panel_y' to 'num_panel'\n",
    "data_m.rename(columns={'num_panel_y': 'num_panel'}, inplace=True)\n",
    "data_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y_s = data_Y_s.sort_index()\n",
    "data_m = data_m.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14268530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Y DataFrame to CSV\n",
    "data_Y_s.to_csv('G:\\\\Mi unidad\\\\PUCP\\\\2021-2\\\\TESIS_1\\\\3_datos\\\\data_Y_s_271223_1.csv', index=False)\n",
    "\n",
    "# Export X DataFrame to CSV\n",
    "data_m.to_csv('G:\\\\Mi unidad\\\\PUCP\\\\2021-2\\\\TESIS_1\\\\3_datos\\\\data_m_271223_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35bd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"flaml[automl]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2dde06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1553cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070e613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b9183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
